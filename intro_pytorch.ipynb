{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moises Quiroz\n",
    "\n",
    "Notebook como introduccion a pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar pytorch\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funcion de activacion\n",
    "def activation(x):\n",
    "    # Sigmoide como funcion de activacion\n",
    "    return 1/(1+torch.exp(-x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genarando algo de data\n",
    "torch.manual_seed(7)\n",
    "\n",
    "#Features 5 variables aleatorias, 1 columna 5 filas\n",
    "features = torch.randn((1,5))\n",
    "\n",
    "#Weights, variables random con la misma distribucion\n",
    "\n",
    "weights = torch.randn_like(features)\n",
    "\n",
    "# El bias\n",
    "\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]]) tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]]) tensor([[0.3177]])\n"
     ]
    }
   ],
   "source": [
    "print(features,weights,bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una prediccion simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1595]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def output(features,weights,bias):\n",
    "    return activation(torch.sum(features*weights)+bias) ##torch.mm(features,weights.t())+bias\n",
    "output(features,weights,bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8948],\n",
       "        [-0.3556],\n",
       "        [ 1.2324],\n",
       "        [ 0.1382],\n",
       "        [-1.6822]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.view(5,1) # Se recomienda usar .view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Output para un multilayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468]])\n",
      "tensor([[-1.1143,  1.6908],\n",
      "        [-0.8948, -0.3556],\n",
      "        [ 1.2324,  0.1382]]) tensor([[-1.6822],\n",
      "        [ 0.3177]])\n"
     ]
    }
   ],
   "source": [
    "### Generando algo de data\n",
    "torch.manual_seed(7)\n",
    "\n",
    "features= torch.randn((1,3))\n",
    "print(features)\n",
    "\n",
    "#Definiendo las dimensiones de cada layer en la red\n",
    "\n",
    "n_input = features.shape[1]\n",
    "n_hidden = 2\n",
    "n_output = 1\n",
    "\n",
    "#weights por input para hidden layer\n",
    "W1 = torch.randn(n_input,n_hidden)\n",
    "W2 = torch.randn(n_hidden,n_output)\n",
    "\n",
    "print(W1,W2)\n",
    "\n",
    "# y el termino bias para el hidden y el output\n",
    "\n",
    "B1 = torch.randn((1,n_hidden))\n",
    "B2 = torch.randn((1,n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3171]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = activation(torch.mm(features,W1)+B1)\n",
    "output = activation(torch.mm(h,W2)+B2)\n",
    "output # Prediccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17862309, 0.55795796, 0.3843166 ],\n",
       "       [0.37868964, 0.46437442, 0.83900209],\n",
       "       [0.28552609, 0.27510307, 0.74960895],\n",
       "       [0.61238366, 0.96887876, 0.14867516]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1786, 0.5580, 0.3843],\n",
       "        [0.3787, 0.4644, 0.8390],\n",
       "        [0.2855, 0.2751, 0.7496],\n",
       "        [0.6124, 0.9689, 0.1487]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17862309, 0.55795796, 0.3843166 ],\n",
       "       [0.37868964, 0.46437442, 0.83900209],\n",
       "       [0.28552609, 0.27510307, 0.74960895],\n",
       "       [0.61238366, 0.96887876, 0.14867516]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "#Define una transformacion para normalizar la data \n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,)),])\n",
    "\n",
    "#Dowload and load the training data\n",
    "\n",
    "trainset = datasets.MNIST('MNIST_data/',download = True,train = True,transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: MNIST_data/\n",
       "    Split: Train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, label = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1, 28, 28])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAADnZJREFUeJzt3X+MVfWZx/HPIwU1tAlgA0xAhEVdtzE61AmalBjWhobdkGCjNZgY2bgpNemEbbImGv8QjWlS14VdUVJDUyxNwLYBf5DabCFmFTBiHHBTodBiYIRZyEwNNVCjIcKzf8xhM8W53zNzzzn33OF5vxIy997nnnue3OEz59z7/d77NXcXgHguq7sBAPUg/EBQhB8IivADQRF+ICjCDwRF+IGgCD8QFOEHgvpSK3dmZkwnBCrm7jaS+xU68pvZYjP7g5l9YGaPFHksAK1lzc7tN7Nxkv4oaZGkPknvSrrX3X+f2IYjP1CxVhz550v6wN2PuPtZSb+QtLTA4wFooSLhnyHp+JDrfdltf8XMVphZj5n1FNgXgJIVecNvuFOLL5zWu/t6SeslTvuBdlLkyN8n6eoh12dKOlGsHQCtUiT870q6zszmmNkEScskbSunLQBVa/q0390/N7NuSb+VNE7SBnc/UFpnACrV9FBfUzvjNT9QuZZM8gEwdhF+ICjCDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoAg/EFRLl+iOauLEicn6rl27kvV58+Yl60eOHGlYmzt3bnJbxMWRHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCKjTOb2a9ks5IOifpc3fvKqOpS83DDz+crN98883Jet5KytOnT29Ye+utt5Lb1mnt2rXJ+ttvv52sHzt2rMx2wiljks/fu/tHJTwOgBbitB8Iqmj4XdJ2M9trZivKaAhAaxQ97f+Gu58ws6mSdpjZIXffOfQO2R8F/jAAbabQkd/dT2Q/ByS9LGn+MPdZ7+5dvBkItJemw29mE83sKxcuS/qWpP1lNQagWkVO+6dJetnMLjzOZnf/r1K6AlA5yxtDLnVnZq3bWQvdeuutyfobb7yRrE+YMCFZz/7ANlTl77DOfff39yfrK1euTNa3bNlSZjtjhrunf2kZhvqAoAg/EBThB4Ii/EBQhB8IivADQTHUV4LFixcn66+99lqhx88bbtu3b1/D2qFDhyrd98yZM5P1BQsWVLbv1FeWS9K1117b9L7HMob6ACQRfiAowg8ERfiBoAg/EBThB4Ii/EBQLNFdgkWLFiXreePVee64445kPe8jw+0qr+/bb789Wb/mmmua3n7nzp0Na1Fw5AeCIvxAUIQfCIrwA0ERfiAowg8ERfiBoBjnL8Ftt92WrBf9zoSxOo6fZ+rUqYW2//DDD5N1xvLTOPIDQRF+ICjCDwRF+IGgCD8QFOEHgiL8QFC54/xmtkHSEkkD7n5jdtsUSb+UNFtSr6R73P3P1bUZW3d3d7L+3HPPtaiT0ZszZ07DWt7n8fMcPXq00PbRjeTI/zNJF69K8Yik1939OkmvZ9cBjCG54Xf3nZJOXXTzUkkbs8sbJd1Zcl8AKtbsa/5p7n5SkrKfxeZpAmi5yuf2m9kKSSuq3g+A0Wn2yN9vZh2SlP0caHRHd1/v7l3u3tXkvgBUoNnwb5O0PLu8XNKr5bQDoFVyw29mL0p6W9Lfmlmfmf2zpB9JWmRmhyUtyq4DGENyX/O7+70NSt8suZcxa+3atcl63uf989x///3Jeur7AtatW1do33lzCO67775kffz48Q1rV1xxRXLbs2fPJuurVq1K1pHGDD8gKMIPBEX4gaAIPxAU4QeCIvxAUHx1dwn6+/uT9c8++yxZv/LKK5P1rq705MhU/dlnn01u285eeOGFZJ2P9BbDkR8IivADQRF+ICjCDwRF+IGgCD8QFOEHgrKiy0ePamdmrdtZG5k9e3ayfuDAgWQ9bx5Alb9DM2vbfX/66afJ+qZNmxrWnn766eS2hw8fTtbbmbunn7gMR34gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIpx/hZILVMtSfv370/Wqxznf++995L1vO8imDx5crJ+ww03jLqnC6qcY/DJJ58k61u3bk3Wn3jiiWS9t7d3tC2VhnF+AEmEHwiK8ANBEX4gKMIPBEX4gaAIPxBU7ji/mW2QtETSgLvfmN32uKTvSvpTdrdH3f03uTu7RMf5Z82alazv2bMnWZ82bVqyXmS8+/nnn09uu3LlymT93LlzyXreMtvTp09P1lMmTZqUrD/00EPJemqOwbx585Lb5j3neXMzbrrppmS9SmWO8/9M0uJhbv8Pd+/M/uUGH0B7yQ2/u++UdKoFvQBooSKv+bvN7HdmtsHM0nM8AbSdZsP/Y0lzJXVKOilpdaM7mtkKM+sxs54m9wWgAk2F39373f2cu5+X9BNJ8xP3Xe/uXe6eXm0SQEs1FX4z6xhy9duS0m99Amg7uUt0m9mLkhZK+qqZ9UlaJWmhmXVKckm9kr5XYY8AKsDn+UuwbNmyZH3z5s2FHv+pp55K1vv6+hrW1q1bV2jfl6pt27Yl60uWLCn0+GvWrEnW8+YoFMHn+QEkEX4gKMIPBEX4gaAIPxAU4QeCYqhvhFIfXd27d29y2yJfXy1J48aNK7Q9Ru/06dPJ+sSJE5P1Q4cOJeu33HJLw1re16XnYagPQBLhB4Ii/EBQhB8IivADQRF+ICjCDwSV+3l+DOro6GhYKzqOv3v37kLbo3yHDx9O1js7O5P1vPkz58+fH3VPZePIDwRF+IGgCD8QFOEHgiL8QFCEHwiK8ANBMc7fBubPb7jgESqycOHCZD1vCe+8cfyPP/44WT979myy3goc+YGgCD8QFOEHgiL8QFCEHwiK8ANBEX4gqNxxfjO7WtLPJU2XdF7Send/xsymSPqlpNmSeiXd4+5/rq7Veh09erRh7cknn0xu+9hjjyXrl19+ebKe9z3uzzzzTMPa6tWrk9sODAwk65equ+++u9D2Zumvxt++fXuhx2+FkRz5P5f0r+7+d5Juk/R9M/uapEckve7u10l6PbsOYIzIDb+7n3T3fdnlM5IOSpohaamkjdndNkq6s6omAZRvVK/5zWy2pHmS3pE0zd1PSoN/ICRNLbs5ANUZ8dx+M/uypK2SfuDup/Ne8wzZboWkFc21B6AqIzrym9l4DQZ/k7u/lN3cb2YdWb1D0rDvHLn7enfvcveuMhoGUI7c8NvgIf6nkg66+5ohpW2SlmeXl0t6tfz2AFQld4luM1sgaZek9zU41CdJj2rwdf+vJM2SdEzSd9z9VM5jjdklulOmTk2/3fHmm28m69dff32ynvcSK/U7PHPmTHLbAwcOJOs7duxI1vfs2ZOsP/DAAw1rM2bMSG5bpa6u9InoZZelj4s9PT3J+tKlS5P1KodYR7pEd+5rfnffLanRg31zNE0BaB/M8AOCIvxAUIQfCIrwA0ERfiAowg8ElTvOX+rOLtFx/jx58wAefPDBZL27uztZv+qqq0bd00gVmWMwlvedNz9i0qRJle27qJGO83PkB4Ii/EBQhB8IivADQRF+ICjCDwRF+IGgGOcfA6ZMmZKsd3Z2NqylPk8vSXfddVeynve14u08zr979+6GtS1btiS3feWVV5L148ePJ+t1YpwfQBLhB4Ii/EBQhB8IivADQRF+ICjCDwTFOD9wiWGcH0AS4QeCIvxAUIQfCIrwA0ERfiAowg8ElRt+M7vazP7bzA6a2QEz+5fs9sfN7H/N7H+yf/9YfbsAypI7ycfMOiR1uPs+M/uKpL2S7pR0j6S/uPu/j3hnTPIBKjfSST5fGsEDnZR0Mrt8xswOSppRrD0AdRvVa34zmy1pnqR3spu6zex3ZrbBzCY32GaFmfWYWU+hTgGUasRz+83sy5LelPRDd3/JzKZJ+kiSS3pSgy8Nkl8Yx2k/UL2RnvaPKPxmNl7SryX91t3XDFOfLenX7n5jzuMQfqBipX2wxwa/QvWnkg4ODX72RuAF35a0f7RNAqjPSN7tXyBpl6T3JZ3Pbn5U0r2SOjV42t8r6XvZm4Opx+LID1Ss1NP+shB+oHp8nh9AEuEHgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxAU4QeCIvxAUIQfCIrwA0ERfiCo3C/wLNlHkj4ccv2r2W3tqF17a9e+JHprVpm9XTPSO7b08/xf2LlZj7t31dZAQrv21q59SfTWrLp647QfCIrwA0HVHf71Ne8/pV17a9e+JHprVi291fqaH0B96j7yA6hJLeE3s8Vm9gcz+8DMHqmjh0bMrNfM3s9WHq51ibFsGbQBM9s/5LYpZrbDzA5nP4ddJq2m3tpi5ebEytK1PnfttuJ1y0/7zWycpD9KWiSpT9K7ku5199+3tJEGzKxXUpe71z4mbGa3S/qLpJ9fWA3JzP5N0il3/1H2h3Oyuz/cJr09rlGu3FxRb41Wlv4n1fjclbnidRnqOPLPl/SBux9x97OSfiFpaQ19tD133ynp1EU3L5W0Mbu8UYP/eVquQW9twd1Puvu+7PIZSRdWlq71uUv0VYs6wj9D0vEh1/vUXkt+u6TtZrbXzFbU3cwwpl1YGSn7ObXmfi6Wu3JzK120snTbPHfNrHhdtjrCP9xqIu005PANd/+6pH+Q9P3s9BYj82NJczW4jNtJSavrbCZbWXqrpB+4++k6exlqmL5qed7qCH+fpKuHXJ8p6UQNfQzL3U9kPwckvazBlyntpP/CIqnZz4Ga+/l/7t7v7ufc/bykn6jG5y5bWXqrpE3u/lJ2c+3P3XB91fW81RH+dyVdZ2ZzzGyCpGWSttXQxxeY2cTsjRiZ2URJ31L7rT68TdLy7PJySa/W2MtfaZeVmxutLK2an7t2W/G6lkk+2VDGf0oaJ2mDu/+w5U0Mw8z+RoNHe2nwE4+b6+zNzF6UtFCDn/rql7RK0iuSfiVplqRjkr7j7i1/461Bbws1ypWbK+qt0crS76jG567MFa9L6YcZfkBMzPADgiL8QFCEHwiK8ANBEX4gKMIPBEX4gaAIPxDU/wEd+HZmFLeFpgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imag = images.view(images.shape[0], -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = Imag.shape[1]\n",
    "n_hidden = 256\n",
    "n_output = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn(n_input,n_hidden)\n",
    "W2 = torch.randn(n_hidden,n_output)\n",
    "B1 = torch.randn(n_hidden)\n",
    "B2 = torch.randn(n_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = activation(torch.mm(Imag,W1)+B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = activation(torch.mm(h,W2)+B2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6977,  0.4817,  0.7772,  ...,  1.0160,  0.3812,  2.1565],\n",
       "        [ 1.1546,  0.1195,  1.8899,  ...,  0.7064,  4.7487,  0.4544],\n",
       "        [ 0.1213,  1.4656,  0.3479,  ...,  2.5171,  0.1050,  3.8630],\n",
       "        ...,\n",
       "        [ 2.0608,  0.3045,  0.7462,  ...,  1.3487, 30.3469,  1.1908],\n",
       "        [ 1.3371,  1.1689,  0.1964,  ...,  1.3220,  2.7175,  2.2597],\n",
       "        [ 0.7054,  0.4288,  1.5648,  ...,  2.5338,  0.8202,  0.5277]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x),dim =1).view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0458, 0.1233, 0.1202, 0.1244, 0.1237, 0.0460, 0.1244, 0.1244, 0.0458,\n",
       "         0.1221],\n",
       "        [0.0698, 0.0587, 0.0736, 0.1417, 0.1593, 0.0611, 0.1593, 0.1593, 0.0586,\n",
       "         0.0586],\n",
       "        [0.0512, 0.1349, 0.0743, 0.1388, 0.1385, 0.0859, 0.0839, 0.1388, 0.0511,\n",
       "         0.1026],\n",
       "        [0.0486, 0.0711, 0.1316, 0.1317, 0.1317, 0.0530, 0.1317, 0.1317, 0.0484,\n",
       "         0.1206],\n",
       "        [0.0617, 0.1262, 0.1284, 0.1234, 0.1285, 0.0473, 0.1284, 0.1282, 0.0473,\n",
       "         0.0808],\n",
       "        [0.0517, 0.1406, 0.0724, 0.1395, 0.1404, 0.0518, 0.1392, 0.1406, 0.0517,\n",
       "         0.0721],\n",
       "        [0.0474, 0.1247, 0.1285, 0.1254, 0.1287, 0.0473, 0.1275, 0.1287, 0.0473,\n",
       "         0.0946],\n",
       "        [0.0516, 0.0569, 0.1242, 0.1379, 0.1342, 0.1186, 0.1372, 0.1379, 0.0507,\n",
       "         0.0508],\n",
       "        [0.0550, 0.1421, 0.0759, 0.1436, 0.1423, 0.0528, 0.1379, 0.1430, 0.0528,\n",
       "         0.0545],\n",
       "        [0.0568, 0.0568, 0.0571, 0.1538, 0.1541, 0.0568, 0.1055, 0.1541, 0.0567,\n",
       "         0.1484],\n",
       "        [0.0519, 0.1409, 0.1376, 0.0753, 0.1410, 0.0680, 0.1409, 0.1406, 0.0519,\n",
       "         0.0519],\n",
       "        [0.0558, 0.1399, 0.1106, 0.1460, 0.1459, 0.0539, 0.0944, 0.1460, 0.0537,\n",
       "         0.0538],\n",
       "        [0.0504, 0.1219, 0.1320, 0.1331, 0.0794, 0.0533, 0.1160, 0.1331, 0.0489,\n",
       "         0.1320],\n",
       "        [0.0480, 0.1294, 0.1238, 0.1297, 0.1298, 0.1297, 0.0773, 0.1298, 0.0477,\n",
       "         0.0548],\n",
       "        [0.0540, 0.1466, 0.1368, 0.0611, 0.1466, 0.0540, 0.1446, 0.1467, 0.0540,\n",
       "         0.0557],\n",
       "        [0.0514, 0.1235, 0.1345, 0.1327, 0.1285, 0.0613, 0.1345, 0.1344, 0.0495,\n",
       "         0.0497],\n",
       "        [0.0491, 0.0697, 0.1323, 0.1334, 0.1333, 0.0505, 0.1315, 0.1334, 0.0491,\n",
       "         0.1177],\n",
       "        [0.0628, 0.1545, 0.1544, 0.0572, 0.1549, 0.0570, 0.1548, 0.0905, 0.0570,\n",
       "         0.0570],\n",
       "        [0.0465, 0.1263, 0.1263, 0.1263, 0.1263, 0.0475, 0.1263, 0.1263, 0.0465,\n",
       "         0.1019],\n",
       "        [0.0663, 0.1435, 0.0530, 0.1435, 0.0577, 0.0529, 0.1434, 0.1435, 0.0528,\n",
       "         0.1435],\n",
       "        [0.0460, 0.1171, 0.1246, 0.1243, 0.1249, 0.0460, 0.1245, 0.1249, 0.0459,\n",
       "         0.1219],\n",
       "        [0.0482, 0.1308, 0.0749, 0.1308, 0.0567, 0.1182, 0.1305, 0.1308, 0.0481,\n",
       "         0.1308],\n",
       "        [0.0518, 0.0872, 0.1394, 0.1394, 0.1394, 0.0513, 0.0802, 0.1394, 0.0513,\n",
       "         0.1207],\n",
       "        [0.0488, 0.1245, 0.1326, 0.1326, 0.0584, 0.0574, 0.1326, 0.1324, 0.0488,\n",
       "         0.1319],\n",
       "        [0.0491, 0.1285, 0.1328, 0.1225, 0.1272, 0.0748, 0.1333, 0.1334, 0.0491,\n",
       "         0.0493],\n",
       "        [0.0481, 0.1280, 0.1306, 0.1297, 0.1308, 0.0481, 0.1299, 0.1308, 0.0481,\n",
       "         0.0758],\n",
       "        [0.0460, 0.1211, 0.1237, 0.1242, 0.1242, 0.0492, 0.1218, 0.1243, 0.0457,\n",
       "         0.1197],\n",
       "        [0.0456, 0.1237, 0.1238, 0.1238, 0.1102, 0.1237, 0.1237, 0.1238, 0.0455,\n",
       "         0.0561],\n",
       "        [0.0430, 0.1169, 0.1168, 0.1170, 0.1160, 0.1140, 0.0993, 0.1170, 0.0430,\n",
       "         0.1169],\n",
       "        [0.0493, 0.0952, 0.0991, 0.1232, 0.1340, 0.0497, 0.1322, 0.1340, 0.0493,\n",
       "         0.1339],\n",
       "        [0.0492, 0.0500, 0.1337, 0.1337, 0.1336, 0.0498, 0.1337, 0.1337, 0.0492,\n",
       "         0.1334],\n",
       "        [0.0589, 0.0706, 0.1544, 0.1437, 0.0886, 0.0591, 0.1549, 0.1549, 0.0570,\n",
       "         0.0580],\n",
       "        [0.0620, 0.0555, 0.0737, 0.1452, 0.0535, 0.1296, 0.1380, 0.1448, 0.0534,\n",
       "         0.1444],\n",
       "        [0.0494, 0.1337, 0.1336, 0.0598, 0.1334, 0.0503, 0.1312, 0.1337, 0.0492,\n",
       "         0.1258],\n",
       "        [0.0456, 0.1240, 0.1240, 0.1237, 0.1227, 0.0456, 0.1232, 0.1241, 0.0456,\n",
       "         0.1214],\n",
       "        [0.0522, 0.0522, 0.1391, 0.1418, 0.1229, 0.0527, 0.1032, 0.1419, 0.0522,\n",
       "         0.1418],\n",
       "        [0.0561, 0.1310, 0.1310, 0.1310, 0.1309, 0.0482, 0.1310, 0.1309, 0.0482,\n",
       "         0.0616],\n",
       "        [0.0553, 0.0650, 0.0618, 0.1501, 0.1502, 0.0553, 0.1079, 0.1502, 0.0553,\n",
       "         0.1489],\n",
       "        [0.0859, 0.0993, 0.1100, 0.1112, 0.1111, 0.1097, 0.1110, 0.1111, 0.0409,\n",
       "         0.1099],\n",
       "        [0.0595, 0.1034, 0.0597, 0.1519, 0.1527, 0.0562, 0.1459, 0.1527, 0.0562,\n",
       "         0.0618],\n",
       "        [0.0459, 0.0942, 0.1239, 0.1244, 0.1246, 0.0767, 0.1246, 0.1246, 0.0458,\n",
       "         0.1153],\n",
       "        [0.0458, 0.1148, 0.1214, 0.1228, 0.1196, 0.0706, 0.1152, 0.1228, 0.0452,\n",
       "         0.1219],\n",
       "        [0.0536, 0.1450, 0.0567, 0.1452, 0.1410, 0.0553, 0.1456, 0.1456, 0.0536,\n",
       "         0.0584],\n",
       "        [0.0502, 0.1364, 0.0999, 0.1364, 0.1173, 0.0718, 0.0651, 0.1364, 0.0502,\n",
       "         0.1363],\n",
       "        [0.0535, 0.1453, 0.0564, 0.1453, 0.1447, 0.0542, 0.0564, 0.1453, 0.0535,\n",
       "         0.1453],\n",
       "        [0.0548, 0.1489, 0.0548, 0.1488, 0.1204, 0.0588, 0.1477, 0.1483, 0.0548,\n",
       "         0.0629],\n",
       "        [0.0456, 0.1235, 0.1224, 0.1238, 0.1238, 0.0456, 0.1234, 0.1238, 0.0456,\n",
       "         0.1225],\n",
       "        [0.0498, 0.1282, 0.1337, 0.1354, 0.1353, 0.0499, 0.1282, 0.1354, 0.0498,\n",
       "         0.0543],\n",
       "        [0.0529, 0.0518, 0.1311, 0.1380, 0.1380, 0.0529, 0.1373, 0.1380, 0.0508,\n",
       "         0.1092],\n",
       "        [0.0503, 0.1368, 0.1251, 0.1351, 0.1367, 0.0503, 0.1278, 0.1368, 0.0503,\n",
       "         0.0507],\n",
       "        [0.0493, 0.0494, 0.1338, 0.1339, 0.1335, 0.0494, 0.1338, 0.1339, 0.0493,\n",
       "         0.1337],\n",
       "        [0.0559, 0.1004, 0.0995, 0.1358, 0.1420, 0.0781, 0.1413, 0.1403, 0.0523,\n",
       "         0.0545],\n",
       "        [0.0520, 0.1398, 0.1372, 0.0593, 0.1373, 0.0754, 0.1407, 0.1406, 0.0518,\n",
       "         0.0659],\n",
       "        [0.1058, 0.1348, 0.1047, 0.1346, 0.0963, 0.0557, 0.1340, 0.1348, 0.0496,\n",
       "         0.0496],\n",
       "        [0.0457, 0.1236, 0.1236, 0.1235, 0.1242, 0.0459, 0.1242, 0.1240, 0.0457,\n",
       "         0.1196],\n",
       "        [0.0494, 0.0500, 0.1316, 0.1342, 0.1340, 0.0494, 0.1342, 0.1342, 0.0494,\n",
       "         0.1336],\n",
       "        [0.0927, 0.0611, 0.1319, 0.1321, 0.1320, 0.0486, 0.1321, 0.1320, 0.0486,\n",
       "         0.0889],\n",
       "        [0.0507, 0.1323, 0.1358, 0.1313, 0.1341, 0.0519, 0.1282, 0.1358, 0.0500,\n",
       "         0.0500],\n",
       "        [0.0510, 0.0568, 0.1346, 0.1368, 0.1212, 0.1359, 0.1212, 0.1370, 0.0504,\n",
       "         0.0551],\n",
       "        [0.0474, 0.1000, 0.1267, 0.1209, 0.1287, 0.0488, 0.1286, 0.1287, 0.0474,\n",
       "         0.1228],\n",
       "        [0.0489, 0.1326, 0.0966, 0.1327, 0.1278, 0.0502, 0.1327, 0.1327, 0.0488,\n",
       "         0.0969],\n",
       "        [0.0445, 0.1202, 0.1134, 0.1210, 0.1210, 0.0725, 0.1209, 0.1211, 0.0445,\n",
       "         0.1208],\n",
       "        [0.0521, 0.0522, 0.0632, 0.1416, 0.1375, 0.0768, 0.1416, 0.1416, 0.0521,\n",
       "         0.1416],\n",
       "        [0.0474, 0.1237, 0.1282, 0.1289, 0.1175, 0.0474, 0.1289, 0.1289, 0.0474,\n",
       "         0.1016]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = softmax(output)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Input to hidden layer linear transformation\n",
    "        \n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        \n",
    "        #Output layer 10 units - one for each digit\n",
    "        \n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "        #define sigmoid activation and softmax output\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #pass the tensor input's for each of the operation\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Input to hidden layer linear transformation\n",
    "        \n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        \n",
    "        #Output layer 10 units - one for each digit\n",
    "        \n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #pass the tensor input's for each of the operation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        x = F.softmax(self.output(x),dim = 1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #Input to hidden layer linear transformation\n",
    "        self.hidden = nn.Linear(784,128)\n",
    "        self.hidden2 = nn.Linear(128,64)\n",
    "        #Output layer 10 units - one for each digit\n",
    "        self.output = nn.Linear(64,10)\n",
    "    def forward(self,x):\n",
    "        #pass the tensor input's for each of the operation\n",
    "        x = F.relu(self.hidden(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.softmax(self.output(x),dim = 1)\n",
    "        x = F.cross_entropy(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network2(\n",
       "  (hidden): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (hidden2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2855, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.007931993460096\n",
      "Training loss: 0.9067266142126847\n",
      "Training loss: 0.525845499053947\n",
      "Training loss: 0.4324361214092545\n",
      "Training loss: 0.3873966612350712\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0], -1)\n",
    "    \n",
    "        # TODO: Training pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def test_network(net, trainloader):\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # Create Variables for the inputs and targets\n",
    "    inputs = Variable(images)\n",
    "    targets = Variable(images)\n",
    "\n",
    "    # Clear the gradients from all Variables\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass, then backward pass, then update weights\n",
    "    output = net.forward(inputs)\n",
    "    loss = criterion(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def view_recon(img, recon):\n",
    "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
    "        reconstruction also a PyTorch Tensor\n",
    "    '''\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "    axes[0].imshow(img.numpy().squeeze())\n",
    "    axes[1].imshow(recon.data.numpy().squeeze())\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box-forced')\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAADjCAYAAADQWoDbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFgVJREFUeJzt3Xu8VWWdx/HvlwOoiIIKOMbF4wVJxTQlkyzLS2VYUGYFXkqnkTLxklba2KuamiYn00lHm2LyfhdLQ8uUEtRmxAS8cZEZNJSLCiKCQF4O/OaPvWh2x7XOBQ5rPQc+79frvNz7WevZ67sPx/Pbz7Oes5YjQgAApKZL1QEAAMhDgQIAJIkCBQBIEgUKAJAkChQAIEkUKABAkihQADY529+1fUPVOTaE7Wts//MG9m3xfdueZftDzfe1Pcj2KtsNGxR6M0GBAtAhbB9ve1r2i/UF2/fYfn9FWcL26izLItuXpPjLPiL2jYgpOe3PR0TPiFgrSban2P6H0gNWjAIFYKPZPkfSTyT9i6SdJQ2S9FNJoyqMtX9E9JR0pKTjJZ3afAfbXUtPhTajQAHYKLZ7SfqepNMj4lcRsToi3oqIuyLi6wV9Jth+0fYK2w/a3rdu2wjbs22/lo1+vpa197F9t+1Xbb9i+yHbrf4Oi4inJT0kaWj2OvNtn2f7SUmrbXe1vXc2Snk1m3Yb2exl+tielGV6wPaudXkvtb3A9krb021/oFnfrW3fmvWdYXv/ur7zbR+V8/1pzEaBXW3/QNIHJF2ejQgvt32F7Yub9bnL9tmtfT86EwoUgI01XNLWku5oR597JA2W1E/SDEk31m27UtKXImI71YrK/Vn7uZIWSuqr2ijtHyW1eq022/uo9gv+sbrmMZKOkdRbkiXdJem+LM8Zkm60PaRu/xMkfV9SH0mPN8v7qKQDJO0o6SZJE2xvXbd9lKQJddvvtN2ttdzrRcQFqhXYcdm03zhJ10oas75A2+6j2kjx5ra+bmdAgQKwsXaS9HJENLW1Q0RcFRGvRcQbkr4raf9sJCZJb0nax/b2EbE8ImbUte8iaddshPZQtHwx0Rm2l6tWfH4h6eq6bZdFxIKI+IukQyT1lHRhRLwZEfdLulu1IrbebyLiwSzvBZKG2x6YvZcbImJZRDRFxMWStpJUX9ymR8TtEfGWpEtUK+aHtPV7lSci/iRphWpFSZJGS5oSES9tzOumhgIFYGMtU20KrE3nc2w32L7Q9jO2V0qan23qk/3305JGSHoum04bnrVfJGmepPtsP2v7/FYOdWBE7BARe0TEtyJiXd22BXWP3yFpQbPtz0nqn7d/RKyS9ErWT7bPtT0nm658VVKvuvfSvO861UaB72gle1tcK+nE7PGJkq7vgNdMCgUKwMZ6WNLrkj7Zxv2PV23a6yjVfpk3Zu2WpIh4NCJGqTbddqek27L21yLi3IjYXdInJJ1j+0htmPqR12JJA5udzxokaVHd84HrH9juqdp03eLsfNN5kj4raYeI6K3ayMYFfbtIGpAdc0PzrneDpFHZOa29VftebVYoUAA2SkSskPRtSVfY/qTtHra72f6Y7R/ldNlO0huqjbx6qLbyT5Jku7vtE2z3yqbEVkpav9T647b3tO269rUd8BYekbRa0jey3B9SrQDeUrfPCNvvt91dtXNRj0TEguy9NElaKqmr7W9L2r7Z6x9k+9hshHl29t6ntjPjS5J2r2+IiIWqnf+6XtIvs+nKzQoFCsBGi4hLJJ0j6Vuq/bJeIGmc8j/VX6faFNoiSbP19l/WJ0man03/fVn/P401WNLvJa1SbdT207y/IdqA7G9KGinpY5JeVm15/Oez1X/r3STpO6pN7R2k2qIJSbpXtQUf/5O9p9f1t9OHkvRrSZ+TtDx7b8dmxbc9LpV0nO3lti+ra79W0n7aDKf3JMncsBAAOifbh6k21dfY7BzaZoERFAB0QtlS9bMk/WJzLE4SBQoAOh3be0t6VbVl9z+pOM4mwxQfACBJpV6H6sNdPkM1xGZn0roJbn0vAO3FFB8AIElcyRdIXJ8+faKxsbHqGECHmT59+ssR0be1/ShQQOIaGxs1bdq0qmMAHcb2c23Zjyk+AECSKFAAgCRRoAAASaJAAQCSRIECACSJAgUASBIFCgCQJAoUACBJFCgAQJIoUEDJbJ9le6btWbbPrjoPkCoKFFAi20MlnSrpYEn7S/q47cHVpgLSRIECyrW3pKkRsSYimiQ9IOlTFWcCkkSBAso1U9Jhtney3UPSCEkDK84EJImrmQMliog5tv9V0iRJqyQ9Iamp+X62x0oaK0mDBg0qNSOQCkZQQMki4sqIODAiDpP0iqT/zdlnfEQMi4hhffu2etscYLPECAoome1+EbHE9iBJx0oaXnUmIEUUKKB8v7S9k6S3JJ0eEcurDgSkiAIFlCwiPlB1BqAz4BwUACBJFCgAQJIoUACAJFGgAABJokABAJJEgQIAJIkCBQBIEgUKKJntr2b3gppp+2bbW1edCUgRBQooke3+ks6UNCwihkpqkDS62lRAmihQQPm6StrGdldJPSQtrjgPkCQKFFCiiFgk6ceSnpf0gqQVEXFftamANFGggBLZ3kHSKEm7SXqHpG1tn5iz31jb02xPW7p0adkxgSRQoIByHSXpzxGxNCLekvQrSe9rvhP3gwIoUEDZnpd0iO0eti3pSElzKs4EJIkCBZQoIh6RdLukGZKeUu3/wfGVhgISxf2ggJJFxHckfafqHEDqGEEBAJLECAp/1bDTjrntC7/wzsI+r+3VlNs+ZK9FhX3uGjIxt72bGwr7vBVrC7cVGXrNuNz2PS6ZW9hn7bJX2n0cAJsGIygAQJIoUACAJFGgAABJokABAJJEgQIAJIlVfJspH7Rvbvu8c7oX9hl3wOTc9tN639shmdZbV9B+35ri2yL1a3gjt33v7sWfsZ48+bLc9nfudHphn72+/KfCbQDKxQgKKJHtIbYfr/taafvsqnMBKWIEBZQoIuZKOkCSbDdIWiTpjkpDAYliBAVU50hJz0TEc1UHAVJEgQKqM1rSzVWHAFJFgQIqYLu7pJGSJhRs54aF2OJRoIBqfEzSjIh4KW8jNywEWCTRKXR5V/7FWrf69+WFfX7UmH+LoV27Fi8z71LweeWiZUML+0xdvltu+4Jbdy/s0+Pl/IXmvWYVv5+mXvlL0FcP2Kawz8oxK3Pbez6TxI/9GDG9B7SIERRQMts9JH1Ytdu9AyiQxEdJYEsSEWsk7VR1DiB1jKAAAEmiQAEAkkSBAgAkiXNQifjLqIMLtx3/w9/ktp/Sa34Lr5i/Wu+y5cW3b7/+6o/mtg+88ZnCPk0vvpjb3k/57S1p6abuLmjv2UKfnre1OwKAhDCCAgAkiREUkLinFq1Q4/n5o+i2mH/hMR2YBigPIygAQJIoUEDJbPe2fbvtp23PsT286kxAipjiA8p3qaTfRcRx2UVje1QdCEgRBQooke3tJR0m6WRJiog3Jb1ZZSYgVRSokj0/Yb/c9nvfe0lhn50btsptv2NVv8I+P/zZmNz2/tfMKeyzy/L/zm1vKuyBDbC7pKWSrra9v6Tpks6KiNXVxgLSwzkooFxdJR0o6T8i4t2SVks6v/lO9feDWrtmRdkZgSRQoIByLZS0MCIeyZ7frlrB+hv194Nq6NGr1IBAKihQQIki4kVJC2wPyZqOlDS7wkhAsjgHBZTvDEk3Ziv4npV0SsV5gCRRoICSRcTjkoZVnQNIHQVqI3Tpkf/nKx99dHFhn9N7X5Pb3s3Flz1dse4vue0/Pe8zhX3+7s78FXktXZAVAFLCOSgAQJIYQQGJ269/L03jgq/YAjGCAgAkiQIFAEgSU3xA4jb2flBoO+6dlRZGUACAJDGC2ggv/v0Bue2n9X6gsM+6gvb71mxd2OeMW0/LbW+88+HCPgDQ2VGggJLZni/pNdX+LK0pIvijXSAHBQqoxuER8XLVIYCUcQ4KAJAkChRQvpB0n+3ptsdWHQZIFVN8QPkOjYjFtvtJmmT76Yh4sH6HrHCNlaSG7ftWkRGoHAWqFX/+4fDCbTeMvrTdr1d0m/brjv1IYZ/GWazW25xExOLsv0ts3yHpYEkPNttnvKTxkrTVLoOj9JBAApjiA0pke1vb261/LOkjkmZWmwpIEyMooFw7S7rDtlT7/++miPhdtZGANFGggBJFxLOS9q86B9AZMMUHAEgSIyggcdwPClsqRlAAgCQxgmpF9yErC7ft3739r3fRxaNz2/uwlBwA/gYjKABAkihQQOK4YSG2VBQoAECSKFBABWw32H7M9t1VZwFSRYECqnGWpDlVhwBSxiq+zLxLDsltf2DYj1votVVu67uuPrOwxx4Tns5tX9vCUbB5sT1A0jGSfiDpnIrjAMliBAWU7yeSviFpXdVBgJRRoIAS2f64pCURMb2V/cbanmZ72to1K0pKB6SFAgWU61BJI23Pl3SLpCNs39B8p4gYHxHDImJYQ49eZWcEkkCBAkoUEd+MiAER0ShptKT7I+LEimMBSaJAAQCSxCo+oCIRMUXSlIpjAMmiQGUOHT47t33nhm0K+9yxesfc9j2veqGwT9Py5e0LVqKuu+2a277sim6Ffc7Zc1Ju+3n3f66wz94XzMttX7vslRbSAdjSMMUHAEgSBQpI3H79e2k+NyzEFogCBQBIEgUKAJAkChQAIEms4stMnbxvbvu6L/yhsM+obV/ObT/va/0K++x58w657QsPL14t2HBAOZe6eey91+W2r9uAS8Zd2pj/vZGkeP2Ndr8egC0PIygAQJIoUECJbG9t+0+2n7A9y/Y/VZ0JSBVTfEC53pB0RESsst1N0h9t3xMRU6sOBqSGAgWUKCJC0qrsabfsK6pLBKSLKT6gZLYbbD8uaYmkSRHxSNWZgBRRoICSRcTaiDhA0gBJB9se2nyf+hsWLl26tPyQQAKY4ssMvuzZ3PbJn+1Z2OfwbVbltj896oriA43Kb+7SwmeFDVnmvWHa/3nlg0+MyW3f8aTiC7+uXb263cfZHEXEq7anSDpa0sxm28ZLGi9Jw4YNYwoQWyRGUECJbPe13Tt7vI2koyQ9XW0qIE2MoIBy7SLpWtsNqn1AvC0i7q44E5AkChRQooh4UtK7q84BdAZM8QEAkkSBAgAkiSm+TNOLL+W2//jUEwr7nDuu/Rc9vfc9P89tb+nW8l9ZcHhu+9RF+bdol6T3DZif2375gCmFfYoc8VTx7duLVutx+3YAG4sRFAAgSRQoAECSKFAAgCRRoAAASaJAASWyPdD2ZNtzsvtBnVV1JiBVrOIDytUk6dyImGF7O0nTbU+KiNlVBwNSQ4FqRcPkGYXbBkxu/+t9aeDoggO1cLHYpcty25u+uX1hn7MP+n3BluJ/8k/MHZnbvv2n85fgS1z4tb0i4gVJL2SPX7M9R1J/SRQooBmm+ICK2G5U7bJH3A8KyEGBAipgu6ekX0o6OyJW5mznflDY4lGggJLZ7qZacboxIn6Vt09EjI+IYRExrG/fvuUGBBJBgQJKZNuSrpQ0JyIuqToPkDIKFFCuQyWdJOkI249nXyOqDgWkiFV8JWtasDC33V2L/ylWjToot33mKZe3cKTuua1FK/UkKY5YlN/ewlHQPhHxR0muOgfQGTCCAgAkiQIFAEgSBQoAkCQKFAAgSRQoAECSKFAAgCSxzDwRi888uHDbo+demtu+roXX2+/6M3Pb9/j+k4V9WE4OICWMoAAASaJAASWyfZXtJbZnVp0FSB0FCijXNZKOrjoE0BlQoIASRcSDkl6pOgfQGVCgAABJYhVfIr715Rvb3efbS95TuG3Pi+bmtnOL9s7B9lhJYyVp0KBBFacBqsEICkgQNywEKFAAgERRoIAS2b5Z0sOShtheaPuLVWcCUsU5KKBEETGm6gxAZ8EICgCQJAoUACBJTPGV7MWz35fb/qltpxf2Gb9i99z2J096Z2Gfdcuebl8wAEgMIygAQJIoUACAJDHFByTuqUUr1Hj+b6qO0WHmX3hM1RHQSTCCAgAkiQIFlMz20bbn2p5n+/yq8wCpYoqvZN847dbc9k/MHVnYp+GEtbnt615gpV5nY7tB0hWSPixpoaRHbU+MiNnVJgPSwwgKKNfBkuZFxLMR8aakWySNqjgTkCQKFFCu/pIW1D1fmLUBaIYCBZTLOW3xtp3ssban2Z62ds2KEmIB6aFAAeVaKGlg3fMBkhY336n+flANPXqVFg5ICQUKKNejkgbb3s12d0mjJU2sOBOQJFbxASWKiCbb4yTdK6lB0lURMaviWECSKFAlu27IwIItiwr7NG2aKKhIRPxW0m+rzgGkjik+AECSKFAAgCQxxQckbr/+vTSNC6xiC8QICgCQJAoUACBJFCgAQJIoUACAJFGgAABJokABAJJEgQIAJIm/gwISN3369FW251Yco4+kl8lAhg7KsGtbdqJAAembGxHDqgxgexoZyFB2hlIL1KR1E/Ju1gYAwNtwDgoAkCQKFJC+8VUHEBnWI0NNKRkcEWUcBwCAdmEEBQBIEgUKSIDto23PtT3P9vk527eyfWu2/RHbjRVkOMf2bNtP2v6D7TYtFe7IDHX7HWc7bHf4SrK2ZLD92ex7Mcv2TWVnsD3I9mTbj2X/HiM2QYarbC+xPbNgu21flmV80vaBHZ1BEcEXX3xV+CWpQdIzknaX1F3SE5L2abbPVyT9LHs8WtKtFWQ4XFKP7PFpVWTI9ttO0oOSpkoaVsH3YbCkxyTtkD3vV0GG8ZJOyx7vI2n+Jvi5PEzSgZJmFmwfIekeSZZ0iKRHOjoDIyigegdLmhcRz0bEm5JukTSq2T6jJF2bPb5d0pG2O/LPNlrNEBGTI2JN9nSqpAEdePw2Zch8X9KPJL3ewcdva4ZTJV0REcslKSKWVJAhJG2fPe4laXEHZ1BEPCjplRZ2GSXpuqiZKqm37V06MgMFCqhef0kL6p4vzNpy94mIJkkrJO1UcoZ6X1Tt03NHajWD7XdLGhgRd3fwsducQdJekvay/V+2p9o+uoIM35V0ou2Fkn4r6YwOztAW7f2ZaTeuJAFUL28k1Hx5bVv22dQZajvaJ0oaJumDHXj8VjPY7iLp3ySd3MHHbXOGTFfVpvk+pNoo8iHbQyPi1RIzjJF0TURcbHu4pOuzDOs6KENbbOqfSUZQQAIWShpY93yA3j5l89d9bHdVbVqnpemXTZFBto+SdIGkkRHxRgcevy0ZtpM0VNIU2/NVO+8xsYMXSrT13+LXEfFWRPxZ0lzVClaZGb4o6TZJioiHJW2t2vXxytSmn5mNQYECqveopMG2d7PdXbVFEBOb7TNR0heyx8dJuj+yM9VlZcim136uWnHq6PMurWaIiBUR0SciGiOiUbXzYCMjYlpZGTJ3qrZgRLb7qDbl92zJGZ6XdGSWYW/VCtTSDszQFhMlfT5bzXeIpBUR8UJHHoApPqBiEdFke5yke1VbwXVVRMyy/T1J0yJioqQrVZvGmafayGl0BRkuktRT0oRsfcbzETGy5AybVBsz3CvpI7ZnS1or6esRsazkDOdK+k/bX1VtWu3kDv7AIts3qzaN2Sc71/UdSd2yjD9T7dzXCEnzJK2RdEpHHl/iShIAgEQxxQcASBIFCgCQJAoUACBJFCgAQJIoUACAJFGgAABJokABAJJEgQIAJIkCBQBIEgUKAJCk/wMXiMuH1Kv98AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[3].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logps = model(img)\n",
    "\n",
    "# Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "ps = torch.exp(logps)\n",
    "view_classify(img.view(1, 28, 28), ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
